{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda? [CpuDevice(id=0)] cpu\n"
     ]
    }
   ],
   "source": [
    "### IMPORTS ###\n",
    "from typing import Callable, Sequence, Any\n",
    "from functools import partial\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "import jaxopt\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pickle\n",
    "from functions import Fourier, Mixture, Slope, Polynomial, WhiteNoise, Shift\n",
    "from networks import MixtureNeuralProcess, MLP, MeanAggregator, SequenceAggregator, NonLinearMVN, ResBlock\n",
    "print('cuda?', jax.devices(), jax.devices()[0].device_kind)\n",
    "\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"]=\"false\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\".XX\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"]=\"platform\"\n",
    "\n",
    "fixed_seed = 12345\n",
    "rng = jax.random.PRNGKey(fixed_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2500 #actually 5000\n",
    "no_pacing_function = jnp.ones(2500)\n",
    "step_pacing_function = jnp.concatenate([jnp.full(1250, 0.3), jnp.ones(1250)])\n",
    "fixedexp_pacing_function = jnp.concatenate([jnp.full(500, 0.075), jnp.full(500, 0.15), jnp.full(500, 0.3), jnp.full(500, 0.6), jnp.full(500, 1)])#start percentage 0.075, inc 2\n",
    "\n",
    "#the pacing function should contain as a function of the current \"progress\" a percentage of the data that we access\n",
    "def create_curriculum(all_xs_sorted, all_ys_sorted, pacing_function, last_key):\n",
    "    mini_batches = []\n",
    "    for i in (pbar := tqdm.trange(5000 // 2, desc='Creating batches.')):\n",
    "        last_key, subkey = jax.random.split(last_key)\n",
    "        indices1 = jax.random.permutation(last_key, int(pacing_function[i]*len(all_xs_sorted)))[:128]\n",
    "        xs1 = all_xs_sorted[indices1]\n",
    "        ys1 = all_ys_sorted[indices1]\n",
    "        last_key, subkey = jax.random.split(last_key)\n",
    "        indices2 = jax.random.permutation(last_key, int(pacing_function[i]*len(all_xs_sorted)))[:128]\n",
    "        xs2 = all_xs_sorted[indices2]\n",
    "        ys2 = all_ys_sorted[indices2]\n",
    "        mini_batches.append((xs1, ys1, xs2, ys2))\n",
    "    return jnp.asarray(mini_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating batches.: 100%|██████████| 2500/2500 [49:49<00:00,  1.20s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 train batches done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating batches.: 100%|██████████| 2500/2500 [46:53<00:00,  1.13s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 validate batches done.\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 2):\n",
    "    xss_yss_unordered = []\n",
    "    with open(f\"saved_datasets/training_data_{i}.pkl\", \"rb\") as file:\n",
    "        xss_yss_unordered = pickle.load(file)\n",
    "    all_xs_unordered = jnp.concatenate([xs for xs, ys in xss_yss_unordered])\n",
    "    all_ys_unordered = jnp.concatenate([ys for xs, ys in xss_yss_unordered])\n",
    "    unordered_mini_batches_train = create_curriculum(all_xs_unordered, all_ys_unordered, no_pacing_function, rng)\n",
    "    with open(f\"saved_batches/train_no_curr_{i}.pkl\", \"wb\") as file:\n",
    "        pickle.dump(unordered_mini_batches_train, file)\n",
    "    print(f\"{i} train batches done.\")\n",
    "    unordered_mini_batches_validate = create_curriculum(all_xs_unordered, all_ys_unordered, no_pacing_function, rng)\n",
    "    with open(f\"saved_batches/validation_data_{i}.pkl\", \"wb\") as file:\n",
    "        pickle.dump(unordered_mini_batches_validate, file)\n",
    "    print(f\"{i} validate batches done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating batches.: 100%|██████████| 2500/2500 [51:30<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation batches done.\n"
     ]
    }
   ],
   "source": [
    "with open(f\"saved_datasets/evaluation_data.pkl\", \"rb\") as file:\n",
    "    xss_yss_unordered = pickle.load(file)\n",
    "all_xs_unordered = jnp.concatenate([xs for xs, ys in xss_yss_unordered])\n",
    "all_ys_unordered = jnp.concatenate([ys for xs, ys in xss_yss_unordered])\n",
    "unordered_mini_batches_train = create_curriculum(all_xs_unordered, all_ys_unordered, no_pacing_function, rng)\n",
    "with open(f\"saved_batches/evaluation_batches.pkl\", \"wb\") as file:\n",
    "    pickle.dump(unordered_mini_batches_train, file)\n",
    "print(\"evaluation batches done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating batches.: 100%|██████████| 2500/2500 [28:49<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 train batches done.\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    all_xs_ordered = []\n",
    "    all_ys_ordered = []\n",
    "    with open(f\"saved_datasets/ordered_mini_training_data_xs_{i}.pkl\", \"rb\") as file:\n",
    "        all_xs_ordered = pickle.load(file)\n",
    "    with open(f\"saved_datasets/ordered_mini_training_data_ys_{i}.pkl\", \"rb\") as file:\n",
    "        all_ys_ordered = pickle.load(file)\n",
    "    single_step_mini_batches_train = create_curriculum(all_xs_ordered, all_ys_ordered, step_pacing_function, rng)\n",
    "    with open(f\"saved_batches/train_step_curr_{i}.pkl\", \"wb\") as file:\n",
    "        pickle.dump(single_step_mini_batches_train, file)\n",
    "    print(f\"{i} train batches done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 train batches done.\n"
     ]
    }
   ],
   "source": [
    "with open(f\"saved_batches/train_step_curr_{i}.pkl\", \"wb\") as file:\n",
    "    pickle.dump(single_step_mini_batches_train, file)\n",
    "print(f\"{i} train batches done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating batches.: 100%|██████████| 2500/2500 [17:14<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 train batches done.\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    all_xs_ordered = []\n",
    "    all_ys_ordered = []\n",
    "    with open(f\"saved_datasets/ordered_mini_training_data_xs_{i}.pkl\", \"rb\") as file:\n",
    "        all_xs_ordered = pickle.load(file)\n",
    "    with open(f\"saved_datasets/ordered_mini_training_data_ys_{i}.pkl\", \"rb\") as file:\n",
    "        all_ys_ordered = pickle.load(file)\n",
    "    multi_step_mini_batches_train = create_curriculum(all_xs_ordered, all_ys_ordered, fixedexp_pacing_function, rng)\n",
    "    with open(f\"saved_batches/train_fixedexp_curr_{i}.pkl\", \"wb\") as file:\n",
    "        pickle.dump(multi_step_mini_batches_train, file)\n",
    "    print(f\"{i} train batches done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating batches.: 100%|██████████| 2500/2500 [22:01<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 train batches done.\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    all_xs_ordered = []\n",
    "    all_ys_ordered = []\n",
    "    with open(f\"saved_datasets/ordered_normal_training_data_xs_{i}.pkl\", \"rb\") as file:\n",
    "        all_xs_ordered = pickle.load(file)\n",
    "    with open(f\"saved_datasets/ordered_normal_training_data_ys_{i}.pkl\", \"rb\") as file:\n",
    "        all_ys_ordered = pickle.load(file)\n",
    "    multi_step_normal_batches_train = create_curriculum(all_xs_ordered, all_ys_ordered, fixedexp_pacing_function, rng)\n",
    "    with open(f\"saved_batches/bootstrap_fixedexp_curr_{i}.pkl\", \"wb\") as file:\n",
    "        pickle.dump(multi_step_normal_batches_train, file)\n",
    "    print(f\"{i} train batches done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating batches.: 100%|██████████| 2500/2500 [34:29<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 train batches done.\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    all_xs_ordered = []\n",
    "    all_ys_ordered = []\n",
    "    with open(f\"saved_datasets/ordered_normal_training_data_xs_{i}.pkl\", \"rb\") as file:\n",
    "        all_xs_ordered = pickle.load(file)\n",
    "    with open(f\"saved_datasets/ordered_normal_training_data_ys_{i}.pkl\", \"rb\") as file:\n",
    "        all_ys_ordered = pickle.load(file)\n",
    "    single_step_normal_batches_train = create_curriculum(all_xs_ordered, all_ys_ordered, step_pacing_function, rng)\n",
    "    with open(f\"saved_batches/bootstrap_step_curr_{i}.pkl\", \"wb\") as file:\n",
    "        pickle.dump(single_step_normal_batches_train, file)\n",
    "    print(f\"{i} train batches done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralproc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
