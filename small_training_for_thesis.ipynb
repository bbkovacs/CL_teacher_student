{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda? [CpuDevice(id=0)] cpu\n"
     ]
    }
   ],
   "source": [
    "### IMPORTS ###\n",
    "from typing import Callable, Sequence, Any\n",
    "from functools import partial\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "import jaxopt\n",
    "from jax.scipy.stats.norm import logpdf\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pickle\n",
    "from functions import Fourier, Mixture, Slope, Polynomial, WhiteNoise, Shift\n",
    "from networks import MixtureNeuralProcess, MLP, MeanAggregator, SequenceAggregator, NonLinearMVN, ResBlock\n",
    "print('cuda?', jax.devices(), jax.devices()[0].device_kind)\n",
    "\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"]=\"false\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\".XX\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"]=\"platform\"\n",
    "\n",
    "fixed_seed = 12345\n",
    "rng = jax.random.PRNGKey(fixed_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONFIGURATION ###\n",
    "# Test-configuration\n",
    "dataset_size = 128\n",
    "test_resolution = 512\n",
    "\n",
    "# Train-configuration\n",
    "num_posterior_mc = 1  # number of alatents to sample from p(Z | X, Y)\n",
    "batch_size = 128  # number of functions to sample from p(Z)\n",
    "\n",
    "kl_penalty = 1e-4  # Note to self: magnitude of the kl-divergence can take over in the loss\n",
    "num_target_samples = 32\n",
    "num_context_samples = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(\n",
    "    key: flax.typing.PRNGKey, \n",
    "    x: jax.Array, \n",
    "    noise_scale: float = 0.2, \n",
    "    mixture_prob: float = 0.5, \n",
    "    corrupt: bool = True\n",
    "):\n",
    "    noise = jax.random.normal(key, x.shape) * noise_scale\n",
    "    return(-2-jnp.cos(2 * jnp.pi * x)) + corrupt * noise\n",
    "\n",
    "rng, key_data, key_test, key_x = jax.random.split(rng, 4)\n",
    "keys_data = jax.random.split(key_data, (dataset_size,))\n",
    "keys_test = jax.random.split(key_test, (test_resolution,))\n",
    "xs_input = jax.random.uniform(key_x, (dataset_size,)) * 2 - 1\n",
    "ys_input = jax.vmap(f)(keys_data, xs_input)\n",
    "x_test_input = jnp.linspace(-1, 1, test_resolution)\n",
    "y_test = jax.vmap(partial(f, corrupt=False))(keys_test, x_test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTRUCT THE MINI NP MODEL ###\n",
    "embedding_xs = MLP([48, 48], activation=jax.nn.leaky_relu, activate_final=True, use_layernorm=True)\n",
    "embedding_ys = MLP([48, 48], activation=jax.nn.leaky_relu, activate_final=True, use_layernorm=True)\n",
    "embedding_both = MLP([48, 48], activation=jax.nn.leaky_relu, activate_final=True, use_layernorm=True)\n",
    "\n",
    "projection_posterior = NonLinearMVN(MLP([96, 48], activation=jax.nn.leaky_relu, activate_final=False, use_layernorm=True))\n",
    "output_model = nn.Sequential([\n",
    "    ResBlock(\n",
    "        MLP([96, 96], activation=jax.nn.leaky_relu, activate_final=True, use_layernorm=True),\n",
    "    ),\n",
    "    ResBlock(\n",
    "        MLP([96, 96], activation=jax.nn.leaky_relu, activate_final=True, use_layernorm=True),\n",
    "    ),\n",
    "    nn.Dense(2)\n",
    "])\n",
    "projection_outputs = NonLinearMVN(output_model)\n",
    "\n",
    "posterior_aggregator = MeanAggregator(projection_posterior)\n",
    "\n",
    "small_model = MixtureNeuralProcess(\n",
    "    embedding_xs, embedding_ys, embedding_both, \n",
    "    posterior_aggregator, \n",
    "    projection_outputs\n",
    ")\n",
    "\n",
    "rng, key = jax.random.split(rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINE THE LOSS ###\n",
    "# Define how the model loss should be computed\n",
    "def posterior_loss(\n",
    "    params: flax.typing.VariableDict,\n",
    "    key: flax.typing.PRNGKey,\n",
    "    batch,\n",
    "    mask = None,\n",
    "    \n",
    "):\n",
    "    X = batch[0]\n",
    "    y = batch[1]\n",
    "    x_test = batch[2]\n",
    "    y_test = batch[3]\n",
    "    #print(X.shape, \" \", y.shape, \" \", x_test.shape, \" \", y_test.shape)\n",
    "    # Compute ELBO over batch of datasets\n",
    "    key_data, key_model = jax.random.split(key)\n",
    "    elbos = jax.vmap(partial(\n",
    "        small_model.apply, \n",
    "        params, \n",
    "        beta=kl_penalty, k=num_posterior_mc, \n",
    "        method=small_model.elbo\n",
    "    ))(\n",
    "        X, y, x_test, y_test, rngs={'default': jax.random.split(key_model, X.shape[0])}\n",
    "    )\n",
    "    ans = -elbos.mean()\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### JIT STEP ###\n",
    "@jax.jit\n",
    "def step(\n",
    "    theta: flax.typing.VariableDict, \n",
    "    opt_state: optax.OptState, \n",
    "    random_key: flax.typing.PRNGKey,\n",
    "    current_batch,\n",
    "    current_mask = None\n",
    ") -> tuple[flax.typing.VariableDict, optax.OptState, jax.Array]:\n",
    "    # Implements a generic SGD Step\n",
    "    #CHANGE DEPENDING ON MODEL OR TEACHER BEING TRAINED!!!!\n",
    "    value, grad = jax.value_and_grad(posterior_loss, argnums=0)(theta, random_key, current_batch, current_mask)\n",
    "    \n",
    "    updates, opt_state = optimizer.update(grad, opt_state, theta)\n",
    "    theta = optax.apply_updates(theta, updates)\n",
    "    #print(\"step2\", current_batch[0], \" \", current_batch[0].shape)\n",
    "    return theta, opt_state, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### JIT SCAN_TRAIN ###\n",
    "def body(carry, batch):\n",
    "    params, opt_state, key = carry\n",
    "    key_carry, key_step = jax.random.split(key)\n",
    "    #print(\"1 \", batch[0].shape, \" \", batch.shape)\n",
    "    X, x_test = jnp.split(batch[0], indices_or_sections=(num_context_samples, ), axis=1)\n",
    "    y, y_test = jnp.split(batch[1], indices_or_sections=(num_context_samples, ), axis=1)\n",
    "    #print(X.shape)\n",
    "    #print(x_test.shape)\n",
    "    #print(y.shape)\n",
    "    #print(y_test.shape)\n",
    "    mask = None\n",
    "    if len(batch) >= 3:\n",
    "        mask = batch[2]\n",
    "    #print(\"bod_shape\", X.shape, \" \", x_test.shape)\n",
    "    params, opt_state, value = step(params, opt_state, key_step, (X,y, x_test,y_test ), mask)\n",
    "\n",
    "    return (params, opt_state, key_carry), value\n",
    "\n",
    "jax.jit\n",
    "def scan_train(params, opt_state, key, batches):\n",
    "    \n",
    "    last, out = jax.lax.scan(body, (params, opt_state, key), batches)\n",
    "\n",
    "    params, opt_state, _ = last\n",
    "    \n",
    "    return params, opt_state, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def cross_entropy_error(model, params, x_context, y_context, x_target, y_target , rng , k):\n",
    "    y_means = jax.vmap(partial(\n",
    "        standard_model.apply, \n",
    "        params, \n",
    "        beta=kl_penalty, k=num_posterior_mc, \n",
    "        method=standard_model.elbo\n",
    "    ))(\n",
    "        x_context, y_context, x_target, y_target, rngs={'default': jax.random.split(rng, x_context.shape[0])}\n",
    "    )\n",
    "    print(y_means.shape)\n",
    "\n",
    "    # Lets compute the log likelihood of the target points given the means and stds\n",
    "\n",
    "    # Ensure y_means and y_stds are squeezed correctly\n",
    "    y_means = jnp.squeeze(y_means, axis=-1) if k > 1 else jnp.squeeze(y_means)\n",
    "    y_stds = jnp.squeeze(y_stds, axis=-1) if k > 1 else jnp.squeeze(y_stds)\n",
    "    full_y = jnp.squeeze(y_target, axis=-1) if k > 1 else jnp.squeeze(y_target) \n",
    "\n",
    "    log_pdf = logpdf(full_y, y_means,y_stds) \n",
    "    return -jnp.mean(log_pdf)'''\n",
    "\n",
    "\n",
    "\n",
    "def RMSE_means(model, params, x_context, y_context, x_target, y_target, rng, k):\n",
    "    #print(x_context.shape)\n",
    "    #print(y_context.shape)\n",
    "    #print(x_target.shape)\n",
    "    #print(params['params']['posterior_fun']['likelihood']['projection']['Dense_0']['kernel'].shape)\n",
    "    #y_means, y_stds = model.apply(params, x_context, y_context, x_target,k=k, rngs={'default': rng}) \n",
    "    #print(\"rmse \", x_context.shape, \" \", y_context.shape, \" \", x_target.shape, \" \", y_target.shape)\n",
    "    y_means = jax.vmap(partial(\n",
    "        small_model.apply, \n",
    "        params, \n",
    "        beta=kl_penalty, k=num_posterior_mc, \n",
    "        method=small_model.elbo\n",
    "    ))(\n",
    "        x_context, y_context, x_target, y_target, rngs={'default': jax.random.split(rng, x_context.shape[0])}\n",
    "    )\n",
    "    \n",
    "    return jnp.sqrt(jnp.mean((y_means - y_target)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAINING ###\n",
    "#mini_batches should have 4 values: xs1, ys1, xs2, ys2 5000 times.\n",
    "#validation = in task, evaluation = out of task\n",
    "def training_optimization_with_curriculum(input_rng, training_mini_batches, validation_mini_batches, evaluation_mini_batches):\n",
    "    # Initialize the Optimization.\n",
    "    scan_chunk = 2\n",
    "    \n",
    "    rng, key = jax.random.split(input_rng)\n",
    "    \n",
    "    #Setup 0\n",
    "    params = params0 = small_model.init(\n",
    "        {'params': key, 'default': key}, \n",
    "        xs_input[:, None], ys_input[:, None], x_test_input[:1, None]\n",
    "    )\n",
    "    optimizer = optax.chain(\n",
    "        optax.clip(.1),\n",
    "        optax.clip_by_global_norm(1.0),\n",
    "        optax.adamw(learning_rate=1e-3, weight_decay=1e-6),\n",
    "    )\n",
    "    opt_state = optimizer.init(params)\n",
    "\n",
    "    best, best_params = jnp.inf, params\n",
    "    losses = list()\n",
    "    in_task_rmse = list()\n",
    "    out_task_rmse = list()\n",
    "    in_task_ece = list()\n",
    "    out_task_ece = list()\n",
    "    for i in (pbar := tqdm.trange(5000 // scan_chunk, desc='Optimizing params. ')):\n",
    "        xs1, ys1, xs2, ys2 = training_mini_batches[i]\n",
    "        batches = jnp.asarray([(xs1, ys1), (xs2, ys2)])\n",
    "        params_new, opt_state, loss_arr = scan_train(params, opt_state, key, batches)\n",
    "\n",
    "        in_xs1, in_ys1, in_xs2, in_ys2 = validation_mini_batches[i]\n",
    "        out_xs1, out_ys1, out_xs2, out_ys2 = evaluation_mini_batches[i]\n",
    "        \n",
    "        k = 1\n",
    "        \n",
    "        if i % 5 == 0:\n",
    "            x_context, x_target = jnp.split(in_xs1, indices_or_sections=(num_context_samples, ), axis=1)\n",
    "            y_context, y_target = jnp.split(in_ys1, indices_or_sections=(num_context_samples, ), axis=1) \n",
    "            in_task_rmse_arr = RMSE_means(small_model, params_new, x_context, y_context, x_target, y_target, rng, k)\n",
    "        #in_task_ece_arr = cross_entropy_error(standard_model, params_new, x_context, y_context, x_target, y_target , rng , k)\n",
    "            x_context, x_target = jnp.split(out_xs2, indices_or_sections=(num_context_samples, ), axis=1)\n",
    "            y_context, y_target = jnp.split(out_ys2, indices_or_sections=(num_context_samples, ), axis=1) \n",
    "            out_task_rmse_arr = RMSE_means(small_model, params_new, x_context, y_context, x_target, y_target, rng, k)\n",
    "        #out_task_ece_arr = cross_entropy_error(standard_model, params_new, x_context, y_context, x_target, y_target , rng , k)\n",
    "            in_task_rmse.append(in_task_rmse_arr)\n",
    "            out_task_rmse.append(out_task_rmse_arr)\n",
    "        #in_task_ece.append(in_task_ece_arr)\n",
    "        #out_task_ece.append(out_task_ece_arr)\n",
    "        \n",
    "        #x_context, x_target = jnp.split(in_xs2, indices_or_sections=(num_context_samples, ), axis=1)\n",
    "        #y_context, y_target = jnp.split(in_ys2, indices_or_sections=(num_context_samples, ), axis=1) \n",
    "        #in_task_rmse_arr = RMSE_means(standard_model, params_new, x_context, y_context, x_target, y_target, rng, k)\n",
    "        #in_task_ece_arr = cross_entropy_error(standard_model, params_new, x_context, y_context, x_target, y_target , rng , k)\n",
    "        #x_context, x_target = jnp.split(out_xs2, indices_or_sections=(num_context_samples, ), axis=1)\n",
    "        #y_context, y_target = jnp.split(out_ys2, indices_or_sections=(num_context_samples, ), axis=1) \n",
    "        #out_task_rmse_arr = RMSE_means(standard_model, params_new, x_context, y_context, x_target, y_target, rng, k)\n",
    "        #out_task_ece_arr = cross_entropy_error(standard_model, params_new, x_context, y_context, x_target, y_target , rng , k)\n",
    "        #in_task_rmse.append(in_task_rmse_arr)\n",
    "        #out_task_rmse.append(out_task_rmse_arr)\n",
    "        #in_task_ece.append(in_task_ece_arr)\n",
    "        #out_task_ece.append(out_task_ece_arr)\n",
    "        \n",
    "        losses.append(loss_arr)\n",
    "\n",
    "        if loss_arr.min() < best:\n",
    "            best = loss_arr.min()\n",
    "            best_params = params_new\n",
    "        \n",
    "        if jnp.isnan(loss_arr).any():\n",
    "            print(\"break\")\n",
    "            break\n",
    "        else:\n",
    "            params = params_new\n",
    "        \n",
    "        pbar.set_description(f'Optimizing params. Loss: {loss_arr.min():.4f}')\n",
    "    params = best_params\n",
    "    losses = jnp.asarray(losses).ravel()\n",
    "    in_task_rmse = jnp.asarray(in_task_rmse).ravel()\n",
    "    out_task_rmse = jnp.asarray(out_task_rmse).ravel()\n",
    "    in_task_ece = jnp.asarray(in_task_ece).ravel()\n",
    "    out_task_ece = jnp.asarray(out_task_ece).ravel()\n",
    "    return params, losses, in_task_rmse, out_task_rmse, #in_task_ece, out_task_ece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 data is loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing params. Loss: 0.8446: 100%|██████████| 2500/2500 [33:02<00:00,  1.26it/s] \n"
     ]
    }
   ],
   "source": [
    "evaluation_mini_batches = 0\n",
    "with open('saved_batches/evaluation_batches.pkl', 'rb') as file:\n",
    "    evaluation_mini_batches = pickle.load(file)\n",
    "    \n",
    "for model_training_number in range(1):\n",
    "    #Load data\n",
    "    training_mini_batches = 0\n",
    "    validation_mini_batches = 0\n",
    "    with open(f'saved_batches/train_no_curr_{model_training_number}.pkl', 'rb') as file:\n",
    "        training_mini_batches = pickle.load(file)\n",
    "    with open(f'saved_batches/validation_data_{model_training_number}.pkl', 'rb') as file:\n",
    "        validation_mini_batches = pickle.load(file)\n",
    "    print(f\"{model_training_number} data is loaded!\")\n",
    "    \n",
    "    #Setup\n",
    "    params = params0 = small_model.init(\n",
    "        {'params': key, 'default': key}, \n",
    "        xs_input[:, None], ys_input[:, None], x_test_input[:1, None]\n",
    "    )\n",
    "    optimizer = optax.chain(\n",
    "        optax.clip(.1),\n",
    "        optax.clip_by_global_norm(1.0),\n",
    "        optax.adamw(learning_rate=1e-3, weight_decay=1e-6),\n",
    "    )\n",
    "    opt_state = optimizer.init(params)\n",
    "    \n",
    "    #Training\n",
    "    trained_params, losses, in_task_rmse, out_task_rmse = training_optimization_with_curriculum(rng, training_mini_batches, validation_mini_batches, evaluation_mini_batches)\n",
    "    \n",
    "    #Save data\n",
    "    with open(f'saved_results/small_nocurr_params_{model_training_number}.pkl', 'wb') as document_to_write:\n",
    "        pickle.dump(trained_params, document_to_write)\n",
    "    with open(f'saved_results/small_nocurr_losses_{model_training_number}.pkl', 'wb') as document_to_write:\n",
    "        pickle.dump(losses, document_to_write)\n",
    "    \n",
    "    with open(f'saved_results/small_nocurr_intask_rmse_{model_training_number}.pkl', 'wb') as document_to_write:\n",
    "        pickle.dump(in_task_rmse, document_to_write)\n",
    "    #with open(f'saved_results/normal_nocurr_intask_ece_{model_training_number}.pkl', 'wb') as document_to_write:\n",
    "    #    pickle.dump(in_task_ece, document_to_write)\n",
    "    with open(f'saved_results/small_nocurr_outtask_rmse_{model_training_number}.pkl', 'wb') as document_to_write:\n",
    "        pickle.dump(out_task_rmse, document_to_write)\n",
    "    #with open(f'saved_results/normal_nocurr_outtask_ece_{model_training_number}.pkl', 'wb') as document_to_write:\n",
    "    #    pickle.dump(out_task_ece, document_to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_loss_curr_eval_teacher(\n",
    "    params: flax.typing.VariableDict,\n",
    "    key: flax.typing.PRNGKey,\n",
    "    xs,\n",
    "    ys\n",
    "):\n",
    "    # Split into context- and target-points.\n",
    "    X, x_test = jnp.split(xs, indices_or_sections=(num_context_samples, ), axis=1)\n",
    "    y, y_test = jnp.split(ys, indices_or_sections=(num_context_samples, ), axis=1)\n",
    "    #print(batch[0].shape, \" \", batch[1].shape, \" \", batch[2].shape,)\n",
    "\n",
    "    #print(\"Shapes of tensors being mapped:\", X.shape, y.shape, x_test.shape, y_test.shape)\n",
    "    # Compute ELBO over batch of datasets\n",
    "    elbos = jax.vmap(partial(\n",
    "        small_model.apply, \n",
    "        params, \n",
    "        beta=kl_penalty, k=num_posterior_mc, \n",
    "        method=small_model.elbo\n",
    "    ))(\n",
    "        X, y, x_test, y_test, rngs={'default': jax.random.split(rng, X.shape[0])}\n",
    "    )\n",
    "    \n",
    "    return -elbos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "###ORDER THE DATA###\n",
    "def sort_for_curriculum(teacher_params, input_rng, xss_yss, posterior_loss_curr_eval_t):\n",
    "    all_xs = jnp.concatenate([xs for xs, ys in xss_yss])\n",
    "    all_ys = jnp.concatenate([ys for xs, ys in xss_yss])\n",
    "    print(\"all_xs and all_ys is ready!\")\n",
    "    #it would be cool if I could add some progress bar here!\n",
    "    diffs = jnp.concatenate([posterior_loss_curr_eval_t(teacher_params, input_rng, xs, ys) for xs, ys in xss_yss])\n",
    "    print(\"difficulties calculated!\")\n",
    "    sorted_indices = jnp.argsort(diffs)\n",
    "    all_xs_sorted = all_xs[sorted_indices] #is this even possible?\n",
    "    all_ys_sorted = all_ys[sorted_indices]\n",
    "    print(\"sorting done!\")\n",
    "    return all_xs_sorted, all_ys_sorted #all_xs_sorted, all_ys_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_xs and all_ys is ready!\n",
      "difficulties calculated!\n",
      "sorting done!\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):    \n",
    "    xss_yss_unordered = []\n",
    "    with open(f\"saved_datasets/training_data_{i}.pkl\", \"rb\") as file:\n",
    "        xss_yss_unordered = pickle.load(file)\n",
    "    #read in trained params\n",
    "    all_xs_teacher_sorted, all_ys_teacher_sorted = sort_for_curriculum(trained_params, rng, xss_yss_unordered, posterior_loss_curr_eval_teacher)\n",
    "    with open(f\"saved_datasets/ordered_small_training_data_xs_{i}.pkl\", \"wb\") as file:\n",
    "        pickle.dump(all_xs_teacher_sorted, file)\n",
    "    with open(f\"saved_datasets/ordered_small_training_data_ys_{i}.pkl\", \"wb\") as file:\n",
    "        pickle.dump(all_ys_teacher_sorted, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralproc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
